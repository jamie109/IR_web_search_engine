## Web搜索引擎 （期末⼤作业）
## 建议退课！ 建议退课！ 建议退课！ 
### 食用指南
1. 先运行spider.py，获取网页内容。

   * 可自行更改`cc_base_url = 'http://cc.nankai.edu.cn'`和`to_use_url_list = ['http://cc.nankai.edu.cn']`中的链接。
   这是你要爬取的网页，可以改成南开电光学院、人工智能学院等等。
   * `if to_use_url_list is not None and mycount < 50`中mycount小于多少可以自定义，这是你想要爬取的网页数目。
   * `sleep(random.randint(2, 3))`中的2和3也可以更改，这是爬完一个网页等待一会儿的时间，谓之”爬虫礼仪也“。（去掉这行代码会快一些。）

2. 运行content_inverted_index.py，对网页文本内容构造倒排索引。
3. 运行page_rank.py，进行链接分析，把结果保存本地文件夹。
4. 运行main.py，进行查询。

### 实验要求和实现思路
#### 1. 网页抓取(10%) 2023.1.30 finish
##### 要求

根据搜索引擎主题选择⽹站进⾏爬取，爬取网页数⽬不限制，注意不要违法，主题不会作为评分标准，不需要太过纠结爬什么。

##### 思路

* 爬取流程
   * 准备
  
   首先需要一个集合`used_url_set`记录已经爬取过的网页，一个列表`to_use_url_list`记录将要爬取的网页。
   初始时`used_url_set`为空，`to_use_url_list`中只有一个网页A。

   * 爬取A 
  
   A的标题，去掉符号，作为存储相关数据（html、文本内容等）的文件名。 
   html文件存到dataset/html_data文件夹下面。 
   该网页的url（例如http://cc.nankai.edu.cn，存到第一行）和文本内容（存到第二行）存在dataset/web_data文件夹下面。
   其命名为'数字_去掉了标点符号的网页标题.html/txt'，数字表示这是第几个爬取的网页。

   A中可能会有很多跳转到其他地方的链接BCD等等，如果以前没有爬过（它们不在`used_url_set`中），要把它们加到`to_use_url_list`中。

   * 爬完A
  
   把A从`to_use_url_list`删掉，加到`used_url_set`中。
   接着，如果`to_use_url_list`不为空，就从中选取一个url接着爬，我用的是第0个。

#### 2. 文本索引

##### 要求

对网页及其锚文本构建索引，可以按锚文本、⽹⻚标题、URL 等域构建索引。

###### 来自chatgpt的回答
1. 使用Python的urllib库获取网页，使用BeautifulSoup解析网页，获取页面中的文本和锚文本；
2. 用正则表达式过滤掉不需要的文本，比如标签，脚本等；
3. 对锚文本和文本分别进行分词，比如使用NLTK库；
4. 将分词结果构建成索引，比如使用倒排索引；
5. 使用索引搜索锚文本和文本，返回搜索结果。

所以，我要先分词，再弄索引？
##### 思路
1. 对于标题的查询

类似hw3，我抄我自己。感谢认真写hw3代码的wjm，让我有代码可抄。
上链接————[空间向量模型](https://github.com/jamie109/IR_VSM)，里面有报告有我写代码的思路。

这个比hw3简单一些，因为它只有标题，不需要进行不同域的组合。

最后把余弦相似度列表转换成字典，id对应余弦相似度。保存到了本地文件dataset/title_cos_sim_dic.pkl。

2. 锚文本 url

还没写，我还不知道去哪里找锚文本呢(lll￢ω￢)
截止2023.2.1 18：10 我还没写锚文本跟url。

3. 文本内容
这里可能需要倒排索引，我昨天写的那个有问题，明天在改。
* 倒排索引
  * 用一个字典inverted_index（某个单词:\[包含这个单词的的文件\]）保存倒排索引的内容。
  * 遍历/dataset/web_data/文件夹下的所有文件（txt文件，存的是网页文本内容）。
  * 对每一个txt文件，获取其内容content，然后去掉标点、英文字母大写改小写，然后分词（掉包，jieba.lcut_for_search(content)），就会得到单词列表。
  * 对每一个单词，把这个txt文件名中的数字添加到inverted_index中这个单词对应的列表中。
  * 注意这里添加的是文件名中的数字，比如0_计算机学院主页.txt，就加0。因为这个0对应着它的url，根据数字就可以返回链接。
* [倒排索引原理](https://cloud.tencent.com/developer/article/1587656)
* content_inverted_index.py，注释里有，懒得写了，bye。

##### 一些重要的点
1. 文件读取顺序
我对于这个网页的索引，是根据爬取它的先后顺序记录的。就是0、1、2、3……，文件名字是`0_计算机学院主页.txt`格式。
我在读取本地txt文件时，它的读取顺序不太对。
解决:
```python
# 按照文件名开头数字排序
file_name_list.sort(key=lambda x: int(x.split('_')[0]))
```
2. id、title、url、文本内容的对应关系。

|dict|function|
|----|----|
|id_url_dic|根据url查找id|
|url_id_dic|根据id查找url|
|title_id_dic|根据id查找title，目前没有用到根据title找id的|

对文本内容，它的文件名就是`id_标题.txt`。

我在想，是不是写一个 IdMap class 会简单一些？
但如果重写，还得改代码，麻烦，我懒得写了。感觉这两种方式差不多，IdMap里面也是一个字典+列表，我这是两个字典。

3. 查询时间过长

一开始我没用倒排索引，查询文本内容非常慢，查一次用时将近50s。使用倒排索引后，速度提升很快，其实还是有改进空间的，
比如对倒排索引的索引单词的遍历，是哈希还是有二叉树等等方法应该可以实现加速，我就用的直接一个一个遍历。
还需要注意的是倒排索引再爬虫结束后，构建一次就可以，每次查询直接调用结果就可以了。
不要把内容查询跟倒排索引写在一个py下，要不然每次查询都要进行一次倒排索引构建，耗时费事还没意义。

#### 3. 链接分析
##### 要求
使⽤PageRank进⾏链接分析，评估⽹⻚权重。
##### 思路
见page_rank.py。

#### 4. 查询服务
